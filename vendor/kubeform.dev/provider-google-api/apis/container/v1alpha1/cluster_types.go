/*
Copyright AppsCode Inc. and Contributors

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

// Code generated by Kubeform. DO NOT EDIT.

package v1alpha1

import (
	base "kubeform.dev/apimachinery/api/v1alpha1"

	core "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	kmapi "kmodules.xyz/client-go/api/v1"
)

// +genclient
// +k8s:openapi-gen=true
// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object
// +kubebuilder:object:root=true
// +kubebuilder:subresource:status
// +kubebuilder:printcolumn:name="Phase",type=string,JSONPath=`.status.phase`

type Cluster struct {
	metav1.TypeMeta   `json:",inline,omitempty"`
	metav1.ObjectMeta `json:"metadata,omitempty"`
	Spec              ClusterSpec   `json:"spec,omitempty"`
	Status            ClusterStatus `json:"status,omitempty"`
}

type ClusterSpec struct {
	ClusterSpec2 `json:",inline"`
	// +optional
	KubeformOutput ClusterSpec2 `json:"kubeformOutput,omitempty" tf:"-"`
}

type ClusterSpecAddonsConfigCloudrunConfig struct {
	Disabled *bool `json:"disabled" tf:"disabled"`
	// +optional
	LoadBalancerType *string `json:"loadBalancerType,omitempty" tf:"load_balancer_type"`
}

type ClusterSpecAddonsConfigHorizontalPodAutoscaling struct {
	Disabled *bool `json:"disabled" tf:"disabled"`
}

type ClusterSpecAddonsConfigHttpLoadBalancing struct {
	Disabled *bool `json:"disabled" tf:"disabled"`
}

type ClusterSpecAddonsConfigNetworkPolicyConfig struct {
	Disabled *bool `json:"disabled" tf:"disabled"`
}

type ClusterSpecAddonsConfig struct {
	// The status of the CloudRun addon. It is disabled by default. Set disabled = false to enable.
	// +optional
	CloudrunConfig *ClusterSpecAddonsConfigCloudrunConfig `json:"cloudrunConfig,omitempty" tf:"cloudrun_config"`
	// The status of the Horizontal Pod Autoscaling addon, which increases or decreases the number of replica pods a replication controller has based on the resource usage of the existing pods. It ensures that a Heapster pod is running in the cluster, which is also used by the Cloud Monitoring service. It is enabled by default; set disabled = true to disable.
	// +optional
	HorizontalPodAutoscaling *ClusterSpecAddonsConfigHorizontalPodAutoscaling `json:"horizontalPodAutoscaling,omitempty" tf:"horizontal_pod_autoscaling"`
	// The status of the HTTP (L7) load balancing controller addon, which makes it easy to set up HTTP load balancers for services in a cluster. It is enabled by default; set disabled = true to disable.
	// +optional
	HttpLoadBalancing *ClusterSpecAddonsConfigHttpLoadBalancing `json:"httpLoadBalancing,omitempty" tf:"http_load_balancing"`
	// Whether we should enable the network policy addon for the master. This must be enabled in order to enable network policy for the nodes. To enable this, you must also define a network_policy block, otherwise nothing will happen. It can only be disabled if the nodes already do not have network policies enabled. Defaults to disabled; set disabled = false to enable.
	// +optional
	NetworkPolicyConfig *ClusterSpecAddonsConfigNetworkPolicyConfig `json:"networkPolicyConfig,omitempty" tf:"network_policy_config"`
}

type ClusterSpecAuthenticatorGroupsConfig struct {
	// The name of the RBAC security group for use with Google security groups in Kubernetes RBAC. Group name must be in format gke-security-groups@yourdomain.com.
	SecurityGroup *string `json:"securityGroup" tf:"security_group"`
}

type ClusterSpecClusterAutoscalingAutoProvisioningDefaults struct {
	// Scopes that are used by NAP when creating node pools.
	// +optional
	OauthScopes []string `json:"oauthScopes,omitempty" tf:"oauth_scopes"`
	// The Google Cloud Platform Service Account to be used by the node VMs.
	// +optional
	ServiceAccount *string `json:"serviceAccount,omitempty" tf:"service_account"`
}

type ClusterSpecClusterAutoscalingResourceLimits struct {
	// Maximum amount of the resource in the cluster.
	// +optional
	Maximum *int64 `json:"maximum,omitempty" tf:"maximum"`
	// Minimum amount of the resource in the cluster.
	// +optional
	Minimum *int64 `json:"minimum,omitempty" tf:"minimum"`
	// The type of the resource. For example, cpu and memory. See the guide to using Node Auto-Provisioning for a list of types.
	ResourceType *string `json:"resourceType" tf:"resource_type"`
}

type ClusterSpecClusterAutoscaling struct {
	// Contains defaults for a node pool created by NAP.
	// +optional
	AutoProvisioningDefaults *ClusterSpecClusterAutoscalingAutoProvisioningDefaults `json:"autoProvisioningDefaults,omitempty" tf:"auto_provisioning_defaults"`
	// Whether node auto-provisioning is enabled. Resource limits for cpu and memory must be defined to enable node auto-provisioning.
	Enabled *bool `json:"enabled" tf:"enabled"`
	// Global constraints for machine resources in the cluster. Configuring the cpu and memory types is required if node auto-provisioning is enabled. These limits will apply to node pool autoscaling in addition to node auto-provisioning.
	// +optional
	ResourceLimits []ClusterSpecClusterAutoscalingResourceLimits `json:"resourceLimits,omitempty" tf:"resource_limits"`
}

type ClusterSpecDatabaseEncryption struct {
	// The key to use to encrypt/decrypt secrets.
	// +optional
	KeyName *string `json:"keyName,omitempty" tf:"key_name"`
	// ENCRYPTED or DECRYPTED.
	State *string `json:"state" tf:"state"`
}

type ClusterSpecDefaultSnatStatus struct {
	// When disabled is set to false, default IP masquerade rules will be applied to the nodes to prevent sNAT on cluster internal traffic.
	Disabled *bool `json:"disabled" tf:"disabled"`
}

type ClusterSpecIpAllocationPolicy struct {
	// The IP address range for the cluster pod IPs. Set to blank to have a range chosen with the default size. Set to /netmask (e.g. /14) to have a range chosen with a specific netmask. Set to a CIDR notation (e.g. 10.96.0.0/14) from the RFC-1918 private networks (e.g. 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16) to pick a specific range to use.
	// +optional
	ClusterIpv4CIDRBlock *string `json:"clusterIpv4CIDRBlock,omitempty" tf:"cluster_ipv4_cidr_block"`
	// The name of the existing secondary range in the cluster's subnetwork to use for pod IP addresses. Alternatively, cluster_ipv4_cidr_block can be used to automatically create a GKE-managed one.
	// +optional
	ClusterSecondaryRangeName *string `json:"clusterSecondaryRangeName,omitempty" tf:"cluster_secondary_range_name"`
	// The IP address range of the services IPs in this cluster. Set to blank to have a range chosen with the default size. Set to /netmask (e.g. /14) to have a range chosen with a specific netmask. Set to a CIDR notation (e.g. 10.96.0.0/14) from the RFC-1918 private networks (e.g. 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16) to pick a specific range to use.
	// +optional
	ServicesIpv4CIDRBlock *string `json:"servicesIpv4CIDRBlock,omitempty" tf:"services_ipv4_cidr_block"`
	// The name of the existing secondary range in the cluster's subnetwork to use for service ClusterIPs. Alternatively, services_ipv4_cidr_block can be used to automatically create a GKE-managed one.
	// +optional
	ServicesSecondaryRangeName *string `json:"servicesSecondaryRangeName,omitempty" tf:"services_secondary_range_name"`
}

type ClusterSpecMaintenancePolicyDailyMaintenanceWindow struct {
	// +optional
	Duration  *string `json:"duration,omitempty" tf:"duration"`
	StartTime *string `json:"startTime" tf:"start_time"`
}

type ClusterSpecMaintenancePolicyMaintenanceExclusion struct {
	EndTime       *string `json:"endTime" tf:"end_time"`
	ExclusionName *string `json:"exclusionName" tf:"exclusion_name"`
	StartTime     *string `json:"startTime" tf:"start_time"`
}

type ClusterSpecMaintenancePolicyRecurringWindow struct {
	EndTime    *string `json:"endTime" tf:"end_time"`
	Recurrence *string `json:"recurrence" tf:"recurrence"`
	StartTime  *string `json:"startTime" tf:"start_time"`
}

type ClusterSpecMaintenancePolicy struct {
	// Time window specified for daily maintenance operations. Specify start_time in RFC3339 format "HH:MMâ€, where HH : [00-23] and MM : [00-59] GMT.
	// +optional
	DailyMaintenanceWindow *ClusterSpecMaintenancePolicyDailyMaintenanceWindow `json:"dailyMaintenanceWindow,omitempty" tf:"daily_maintenance_window"`
	// Exceptions to maintenance window. Non-emergency maintenance should not occur in these windows.
	// +optional
	// +kubebuilder:validation:MaxItems=3
	MaintenanceExclusion []ClusterSpecMaintenancePolicyMaintenanceExclusion `json:"maintenanceExclusion,omitempty" tf:"maintenance_exclusion"`
	// Time window for recurring maintenance operations.
	// +optional
	RecurringWindow *ClusterSpecMaintenancePolicyRecurringWindow `json:"recurringWindow,omitempty" tf:"recurring_window"`
}

type ClusterSpecMasterAuthClientCertificateConfig struct {
	// Whether client certificate authorization is enabled for this cluster.
	IssueClientCertificate *bool `json:"issueClientCertificate" tf:"issue_client_certificate"`
}

type ClusterSpecMasterAuth struct {
	// Base64 encoded public certificate used by clients to authenticate to the cluster endpoint.
	// +optional
	ClientCertificate *string `json:"clientCertificate,omitempty" tf:"client_certificate"`
	// Whether client certificate authorization is enabled for this cluster.
	// +optional
	ClientCertificateConfig *ClusterSpecMasterAuthClientCertificateConfig `json:"clientCertificateConfig,omitempty" tf:"client_certificate_config"`
	// Base64 encoded private key used by clients to authenticate to the cluster endpoint.
	// +optional
	ClientKey *string `json:"-" sensitive:"true" tf:"client_key"`
	// Base64 encoded public certificate that is the root of trust for the cluster.
	// +optional
	ClusterCaCertificate *string `json:"clusterCaCertificate,omitempty" tf:"cluster_ca_certificate"`
	// The password to use for HTTP basic authentication when accessing the Kubernetes master endpoint.
	// +optional
	Password *string `json:"-" sensitive:"true" tf:"password"`
	// The username to use for HTTP basic authentication when accessing the Kubernetes master endpoint. If not present basic auth will be disabled.
	// +optional
	Username *string `json:"username,omitempty" tf:"username"`
}

type ClusterSpecMasterAuthorizedNetworksConfigCidrBlocks struct {
	// External network that can access Kubernetes master through HTTPS. Must be specified in CIDR notation.
	CidrBlock *string `json:"cidrBlock" tf:"cidr_block"`
	// Field for users to identify CIDR blocks.
	// +optional
	DisplayName *string `json:"displayName,omitempty" tf:"display_name"`
}

type ClusterSpecMasterAuthorizedNetworksConfig struct {
	// External networks that can access the Kubernetes cluster master through HTTPS.
	// +optional
	CidrBlocks []ClusterSpecMasterAuthorizedNetworksConfigCidrBlocks `json:"cidrBlocks,omitempty" tf:"cidr_blocks"`
}

type ClusterSpecNetworkPolicy struct {
	// Whether network policy is enabled on the cluster.
	Enabled *bool `json:"enabled" tf:"enabled"`
	// The selected network policy provider. Defaults to PROVIDER_UNSPECIFIED.
	// +optional
	Provider *string `json:"provider,omitempty" tf:"provider"`
}

type ClusterSpecNodeConfigGuestAccelerator struct {
	// The number of the accelerator cards exposed to an instance.
	Count *int64 `json:"count" tf:"count"`
	// The accelerator type resource name.
	Type *string `json:"type" tf:"type"`
}

type ClusterSpecNodeConfigShieldedInstanceConfig struct {
	// Defines whether the instance has integrity monitoring enabled.
	// +optional
	EnableIntegrityMonitoring *bool `json:"enableIntegrityMonitoring,omitempty" tf:"enable_integrity_monitoring"`
	// Defines whether the instance has Secure Boot enabled.
	// +optional
	EnableSecureBoot *bool `json:"enableSecureBoot,omitempty" tf:"enable_secure_boot"`
}

type ClusterSpecNodeConfigTaint struct {
	// Effect for taint.
	Effect *string `json:"effect" tf:"effect"`
	// Key for taint.
	Key *string `json:"key" tf:"key"`
	// Value for taint.
	Value *string `json:"value" tf:"value"`
}

type ClusterSpecNodeConfigWorkloadMetadataConfig struct {
	// NodeMetadata is the configuration for how to expose metadata to the workloads running on the node.
	NodeMetadata *string `json:"nodeMetadata" tf:"node_metadata"`
}

type ClusterSpecNodeConfig struct {
	// Size of the disk attached to each node, specified in GB. The smallest allowed disk size is 10GB.
	// +optional
	DiskSizeGb *int64 `json:"diskSizeGb,omitempty" tf:"disk_size_gb"`
	// Type of the disk attached to each node.
	// +optional
	DiskType *string `json:"diskType,omitempty" tf:"disk_type"`
	// List of the type and count of accelerator cards attached to the instance.
	// +optional
	GuestAccelerator []ClusterSpecNodeConfigGuestAccelerator `json:"guestAccelerator,omitempty" tf:"guest_accelerator"`
	// The image type to use for this node. Note that for a given image type, the latest version of it will be used.
	// +optional
	ImageType *string `json:"imageType,omitempty" tf:"image_type"`
	// The map of Kubernetes labels (key/value pairs) to be applied to each node. These will added in addition to any default label(s) that Kubernetes may apply to the node.
	// +optional
	Labels *map[string]string `json:"labels,omitempty" tf:"labels"`
	// The number of local SSD disks to be attached to the node.
	// +optional
	LocalSsdCount *int64 `json:"localSsdCount,omitempty" tf:"local_ssd_count"`
	// The name of a Google Compute Engine machine type.
	// +optional
	MachineType *string `json:"machineType,omitempty" tf:"machine_type"`
	// The metadata key/value pairs assigned to instances in the cluster.
	// +optional
	Metadata *map[string]string `json:"metadata,omitempty" tf:"metadata"`
	// Minimum CPU platform to be used by this instance. The instance may be scheduled on the specified or newer CPU platform.
	// +optional
	MinCPUPlatform *string `json:"minCPUPlatform,omitempty" tf:"min_cpu_platform"`
	// The set of Google API scopes to be made available on all of the node VMs.
	// +optional
	OauthScopes []string `json:"oauthScopes,omitempty" tf:"oauth_scopes"`
	// Whether the nodes are created as preemptible VM instances.
	// +optional
	Preemptible *bool `json:"preemptible,omitempty" tf:"preemptible"`
	// The Google Cloud Platform Service Account to be used by the node VMs.
	// +optional
	ServiceAccount *string `json:"serviceAccount,omitempty" tf:"service_account"`
	// Shielded Instance options.
	// +optional
	ShieldedInstanceConfig *ClusterSpecNodeConfigShieldedInstanceConfig `json:"shieldedInstanceConfig,omitempty" tf:"shielded_instance_config"`
	// The list of instance tags applied to all nodes.
	// +optional
	Tags []string `json:"tags,omitempty" tf:"tags"`
	// List of Kubernetes taints to be applied to each node.
	// +optional
	Taint []ClusterSpecNodeConfigTaint `json:"taint,omitempty" tf:"taint"`
	// The workload metadata configuration for this node.
	// +optional
	WorkloadMetadataConfig *ClusterSpecNodeConfigWorkloadMetadataConfig `json:"workloadMetadataConfig,omitempty" tf:"workload_metadata_config"`
}

type ClusterSpecNodePoolAutoscaling struct {
	// Maximum number of nodes in the NodePool. Must be >= min_node_count.
	MaxNodeCount *int64 `json:"maxNodeCount" tf:"max_node_count"`
	// Minimum number of nodes in the NodePool. Must be >=0 and <= max_node_count.
	MinNodeCount *int64 `json:"minNodeCount" tf:"min_node_count"`
}

type ClusterSpecNodePoolManagement struct {
	// Whether the nodes will be automatically repaired.
	// +optional
	AutoRepair *bool `json:"autoRepair,omitempty" tf:"auto_repair"`
	// Whether the nodes will be automatically upgraded.
	// +optional
	AutoUpgrade *bool `json:"autoUpgrade,omitempty" tf:"auto_upgrade"`
}

type ClusterSpecNodePoolNodeConfigGuestAccelerator struct {
	// The number of the accelerator cards exposed to an instance.
	Count *int64 `json:"count" tf:"count"`
	// The accelerator type resource name.
	Type *string `json:"type" tf:"type"`
}

type ClusterSpecNodePoolNodeConfigShieldedInstanceConfig struct {
	// Defines whether the instance has integrity monitoring enabled.
	// +optional
	EnableIntegrityMonitoring *bool `json:"enableIntegrityMonitoring,omitempty" tf:"enable_integrity_monitoring"`
	// Defines whether the instance has Secure Boot enabled.
	// +optional
	EnableSecureBoot *bool `json:"enableSecureBoot,omitempty" tf:"enable_secure_boot"`
}

type ClusterSpecNodePoolNodeConfigTaint struct {
	// Effect for taint.
	Effect *string `json:"effect" tf:"effect"`
	// Key for taint.
	Key *string `json:"key" tf:"key"`
	// Value for taint.
	Value *string `json:"value" tf:"value"`
}

type ClusterSpecNodePoolNodeConfigWorkloadMetadataConfig struct {
	// NodeMetadata is the configuration for how to expose metadata to the workloads running on the node.
	NodeMetadata *string `json:"nodeMetadata" tf:"node_metadata"`
}

type ClusterSpecNodePoolNodeConfig struct {
	// Size of the disk attached to each node, specified in GB. The smallest allowed disk size is 10GB.
	// +optional
	DiskSizeGb *int64 `json:"diskSizeGb,omitempty" tf:"disk_size_gb"`
	// Type of the disk attached to each node.
	// +optional
	DiskType *string `json:"diskType,omitempty" tf:"disk_type"`
	// List of the type and count of accelerator cards attached to the instance.
	// +optional
	GuestAccelerator []ClusterSpecNodePoolNodeConfigGuestAccelerator `json:"guestAccelerator,omitempty" tf:"guest_accelerator"`
	// The image type to use for this node. Note that for a given image type, the latest version of it will be used.
	// +optional
	ImageType *string `json:"imageType,omitempty" tf:"image_type"`
	// The map of Kubernetes labels (key/value pairs) to be applied to each node. These will added in addition to any default label(s) that Kubernetes may apply to the node.
	// +optional
	Labels *map[string]string `json:"labels,omitempty" tf:"labels"`
	// The number of local SSD disks to be attached to the node.
	// +optional
	LocalSsdCount *int64 `json:"localSsdCount,omitempty" tf:"local_ssd_count"`
	// The name of a Google Compute Engine machine type.
	// +optional
	MachineType *string `json:"machineType,omitempty" tf:"machine_type"`
	// The metadata key/value pairs assigned to instances in the cluster.
	// +optional
	Metadata *map[string]string `json:"metadata,omitempty" tf:"metadata"`
	// Minimum CPU platform to be used by this instance. The instance may be scheduled on the specified or newer CPU platform.
	// +optional
	MinCPUPlatform *string `json:"minCPUPlatform,omitempty" tf:"min_cpu_platform"`
	// The set of Google API scopes to be made available on all of the node VMs.
	// +optional
	OauthScopes []string `json:"oauthScopes,omitempty" tf:"oauth_scopes"`
	// Whether the nodes are created as preemptible VM instances.
	// +optional
	Preemptible *bool `json:"preemptible,omitempty" tf:"preemptible"`
	// The Google Cloud Platform Service Account to be used by the node VMs.
	// +optional
	ServiceAccount *string `json:"serviceAccount,omitempty" tf:"service_account"`
	// Shielded Instance options.
	// +optional
	ShieldedInstanceConfig *ClusterSpecNodePoolNodeConfigShieldedInstanceConfig `json:"shieldedInstanceConfig,omitempty" tf:"shielded_instance_config"`
	// The list of instance tags applied to all nodes.
	// +optional
	Tags []string `json:"tags,omitempty" tf:"tags"`
	// List of Kubernetes taints to be applied to each node.
	// +optional
	Taint []ClusterSpecNodePoolNodeConfigTaint `json:"taint,omitempty" tf:"taint"`
	// The workload metadata configuration for this node.
	// +optional
	WorkloadMetadataConfig *ClusterSpecNodePoolNodeConfigWorkloadMetadataConfig `json:"workloadMetadataConfig,omitempty" tf:"workload_metadata_config"`
}

type ClusterSpecNodePoolUpgradeSettings struct {
	// The number of additional nodes that can be added to the node pool during an upgrade. Increasing max_surge raises the number of nodes that can be upgraded simultaneously. Can be set to 0 or greater.
	MaxSurge *int64 `json:"maxSurge" tf:"max_surge"`
	// The number of nodes that can be simultaneously unavailable during an upgrade. Increasing max_unavailable raises the number of nodes that can be upgraded in parallel. Can be set to 0 or greater.
	MaxUnavailable *int64 `json:"maxUnavailable" tf:"max_unavailable"`
}

type ClusterSpecNodePool struct {
	// Configuration required by cluster autoscaler to adjust the size of the node pool to the current cluster usage.
	// +optional
	Autoscaling *ClusterSpecNodePoolAutoscaling `json:"autoscaling,omitempty" tf:"autoscaling"`
	// The initial number of nodes for the pool. In regional or multi-zonal clusters, this is the number of nodes per zone. Changing this will force recreation of the resource.
	// +optional
	InitialNodeCount *int64 `json:"initialNodeCount,omitempty" tf:"initial_node_count"`
	// The resource URLs of the managed instance groups associated with this node pool.
	// +optional
	InstanceGroupUrls []string `json:"instanceGroupUrls,omitempty" tf:"instance_group_urls"`
	// Node management configuration, wherein auto-repair and auto-upgrade is configured.
	// +optional
	Management *ClusterSpecNodePoolManagement `json:"management,omitempty" tf:"management"`
	// The maximum number of pods per node in this node pool. Note that this does not work on node pools which are "route-based" - that is, node pools belonging to clusters that do not have IP Aliasing enabled.
	// +optional
	MaxPodsPerNode *int64 `json:"maxPodsPerNode,omitempty" tf:"max_pods_per_node"`
	// The name of the node pool. If left blank, Terraform will auto-generate a unique name.
	// +optional
	Name *string `json:"name,omitempty" tf:"name"`
	// Creates a unique name for the node pool beginning with the specified prefix. Conflicts with name.
	// +optional
	NamePrefix *string `json:"namePrefix,omitempty" tf:"name_prefix"`
	// The configuration of the nodepool
	// +optional
	NodeConfig *ClusterSpecNodePoolNodeConfig `json:"nodeConfig,omitempty" tf:"node_config"`
	// The number of nodes per instance group. This field can be used to update the number of nodes per instance group but should not be used alongside autoscaling.
	// +optional
	NodeCount *int64 `json:"nodeCount,omitempty" tf:"node_count"`
	// The list of zones in which the node pool's nodes should be located. Nodes must be in the region of their regional cluster or in the same region as their cluster's zone for zonal clusters. If unspecified, the cluster-level node_locations will be used.
	// +optional
	NodeLocations []string `json:"nodeLocations,omitempty" tf:"node_locations"`
	// Specify node upgrade settings to change how many nodes GKE attempts to upgrade at once. The number of nodes upgraded simultaneously is the sum of max_surge and max_unavailable. The maximum number of nodes upgraded simultaneously is limited to 20.
	// +optional
	UpgradeSettings *ClusterSpecNodePoolUpgradeSettings `json:"upgradeSettings,omitempty" tf:"upgrade_settings"`
	// The Kubernetes version for the nodes in this pool. Note that if this field and auto_upgrade are both specified, they will fight each other for what the node version should be, so setting both is highly discouraged. While a fuzzy version can be specified, it's recommended that you specify explicit versions as Terraform will see spurious diffs when fuzzy versions are used. See the google_container_engine_versions data source's version_prefix field to approximate fuzzy versions in a Terraform-compatible way.
	// +optional
	Version *string `json:"version,omitempty" tf:"version"`
}

type ClusterSpecPodSecurityPolicyConfig struct {
	// Enable the PodSecurityPolicy controller for this cluster. If enabled, pods must be valid under a PodSecurityPolicy to be created.
	Enabled *bool `json:"enabled" tf:"enabled"`
}

type ClusterSpecPrivateClusterConfigMasterGlobalAccessConfig struct {
	// Whether the cluster master is accessible globally or not.
	Enabled *bool `json:"enabled" tf:"enabled"`
}

type ClusterSpecPrivateClusterConfig struct {
	// Enables the private cluster feature, creating a private endpoint on the cluster. In a private cluster, nodes only have RFC 1918 private addresses and communicate with the master's private endpoint via private networking.
	EnablePrivateEndpoint *bool `json:"enablePrivateEndpoint" tf:"enable_private_endpoint"`
	// When true, the cluster's private endpoint is used as the cluster endpoint and access through the public endpoint is disabled. When false, either endpoint can be used. This field only applies to private clusters, when enable_private_nodes is true.
	// +optional
	EnablePrivateNodes *bool `json:"enablePrivateNodes,omitempty" tf:"enable_private_nodes"`
	// Controls cluster master global access settings.
	// +optional
	MasterGlobalAccessConfig *ClusterSpecPrivateClusterConfigMasterGlobalAccessConfig `json:"masterGlobalAccessConfig,omitempty" tf:"master_global_access_config"`
	// The IP range in CIDR notation to use for the hosted master network. This range will be used for assigning private IP addresses to the cluster master(s) and the ILB VIP. This range must not overlap with any other ranges in use within the cluster's network, and it must be a /28 subnet. See Private Cluster Limitations for more details. This field only applies to private clusters, when enable_private_nodes is true.
	// +optional
	MasterIpv4CIDRBlock *string `json:"masterIpv4CIDRBlock,omitempty" tf:"master_ipv4_cidr_block"`
	// The name of the peering between this cluster and the Google owned VPC.
	// +optional
	PeeringName *string `json:"peeringName,omitempty" tf:"peering_name"`
	// The internal IP address of this cluster's master endpoint.
	// +optional
	PrivateEndpoint *string `json:"privateEndpoint,omitempty" tf:"private_endpoint"`
	// The external IP address of this cluster's master endpoint.
	// +optional
	PublicEndpoint *string `json:"publicEndpoint,omitempty" tf:"public_endpoint"`
}

type ClusterSpecReleaseChannel struct {
	// The selected release channel. Accepted values are:
	// * UNSPECIFIED: Not set.
	// * RAPID: Weekly upgrade cadence; Early testers and developers who requires new features.
	// * REGULAR: Multiple per month upgrade cadence; Production users who need features not yet offered in the Stable channel.
	// * STABLE: Every few months upgrade cadence; Production users who need stability above all else, and for whom frequent upgrades are too risky.
	Channel *string `json:"channel" tf:"channel"`
}

type ClusterSpecResourceUsageExportConfigBigqueryDestination struct {
	// The ID of a BigQuery Dataset.
	DatasetID *string `json:"datasetID" tf:"dataset_id"`
}

type ClusterSpecResourceUsageExportConfig struct {
	// Parameters for using BigQuery as the destination of resource usage export.
	BigqueryDestination *ClusterSpecResourceUsageExportConfigBigqueryDestination `json:"bigqueryDestination" tf:"bigquery_destination"`
	// Whether to enable network egress metering for this cluster. If enabled, a daemonset will be created in the cluster to meter network egress traffic.
	// +optional
	EnableNetworkEgressMetering *bool `json:"enableNetworkEgressMetering,omitempty" tf:"enable_network_egress_metering"`
	// Whether to enable resource consumption metering on this cluster. When enabled, a table will be created in the resource export BigQuery dataset to store resource consumption data. The resulting table can be joined with the resource usage table or with BigQuery billing export. Defaults to true.
	// +optional
	EnableResourceConsumptionMetering *bool `json:"enableResourceConsumptionMetering,omitempty" tf:"enable_resource_consumption_metering"`
}

type ClusterSpecVerticalPodAutoscaling struct {
	// Enables vertical pod autoscaling.
	Enabled *bool `json:"enabled" tf:"enabled"`
}

type ClusterSpecWorkloadIdentityConfig struct {
	// Enables workload identity.
	IdentityNamespace *string `json:"identityNamespace" tf:"identity_namespace"`
}

type ClusterSpec2 struct {
	TerminationPolicy base.TerminationPolicy `json:"terminationPolicy,omitempty" tf:"-"`

	Timeouts *base.ResourceTimeout `json:"timeouts,omitempty" tf:"timeouts"`

	ProviderRef core.LocalObjectReference `json:"providerRef" tf:"-"`

	ID string `json:"id,omitempty" tf:"id,omitempty"`

	SecretRef *core.LocalObjectReference `json:"secretRef,omitempty" tf:"-"`

	// The configuration for addons supported by GKE.
	// +optional
	AddonsConfig *ClusterSpecAddonsConfig `json:"addonsConfig,omitempty" tf:"addons_config"`
	// Configuration for the Google Groups for GKE feature.
	// +optional
	AuthenticatorGroupsConfig *ClusterSpecAuthenticatorGroupsConfig `json:"authenticatorGroupsConfig,omitempty" tf:"authenticator_groups_config"`
	// Per-cluster configuration of Node Auto-Provisioning with Cluster Autoscaler to automatically adjust the size of the cluster and create/delete node pools based on the current needs of the cluster's workload. See the guide to using Node Auto-Provisioning for more details.
	// +optional
	ClusterAutoscaling *ClusterSpecClusterAutoscaling `json:"clusterAutoscaling,omitempty" tf:"cluster_autoscaling"`
	// The IP address range of the Kubernetes pods in this cluster in CIDR notation (e.g. 10.96.0.0/14). Leave blank to have one automatically chosen or specify a /14 block in 10.0.0.0/8. This field will only work for routes-based clusters, where ip_allocation_policy is not defined.
	// +optional
	ClusterIpv4CIDR *string `json:"clusterIpv4CIDR,omitempty" tf:"cluster_ipv4_cidr"`
	// Application-layer Secrets Encryption settings. The object format is {state = string, key_name = string}. Valid values of state are: "ENCRYPTED"; "DECRYPTED". key_name is the name of a CloudKMS key.
	// +optional
	DatabaseEncryption *ClusterSpecDatabaseEncryption `json:"databaseEncryption,omitempty" tf:"database_encryption"`
	// The desired datapath provider for this cluster. By default, uses the IPTables-based kube-proxy implementation.
	// +optional
	DatapathProvider *string `json:"datapathProvider,omitempty" tf:"datapath_provider"`
	// The default maximum number of pods per node in this cluster. This doesn't work on "routes-based" clusters, clusters that don't have IP Aliasing enabled.
	// +optional
	DefaultMaxPodsPerNode *int64 `json:"defaultMaxPodsPerNode,omitempty" tf:"default_max_pods_per_node"`
	// Whether the cluster disables default in-node sNAT rules. In-node sNAT rules will be disabled when defaultSnatStatus is disabled.
	// +optional
	DefaultSnatStatus *ClusterSpecDefaultSnatStatus `json:"defaultSnatStatus,omitempty" tf:"default_snat_status"`
	//  Description of the cluster.
	// +optional
	Description *string `json:"description,omitempty" tf:"description"`
	// Enable Autopilot for this cluster.
	// +optional
	EnableAutopilot *bool `json:"enableAutopilot,omitempty" tf:"enable_autopilot"`
	// Enable Binary Authorization for this cluster. If enabled, all container images will be validated by Google Binary Authorization.
	// +optional
	EnableBinaryAuthorization *bool `json:"enableBinaryAuthorization,omitempty" tf:"enable_binary_authorization"`
	// Whether Intra-node visibility is enabled for this cluster. This makes same node pod to pod traffic visible for VPC network.
	// +optional
	EnableIntranodeVisibility *bool `json:"enableIntranodeVisibility,omitempty" tf:"enable_intranode_visibility"`
	// Whether to enable Kubernetes Alpha features for this cluster. Note that when this option is enabled, the cluster cannot be upgraded and will be automatically deleted after 30 days.
	// +optional
	EnableKubernetesAlpha *bool `json:"enableKubernetesAlpha,omitempty" tf:"enable_kubernetes_alpha"`
	// Whether the ABAC authorizer is enabled for this cluster. When enabled, identities in the system, including service accounts, nodes, and controllers, will have statically granted permissions beyond those provided by the RBAC configuration or IAM. Defaults to false.
	// +optional
	EnableLegacyAbac *bool `json:"enableLegacyAbac,omitempty" tf:"enable_legacy_abac"`
	// Enable Shielded Nodes features on all nodes in this cluster.
	// +optional
	EnableShieldedNodes *bool `json:"enableShieldedNodes,omitempty" tf:"enable_shielded_nodes"`
	// Whether to enable Cloud TPU resources in this cluster.
	// +optional
	EnableTpu *bool `json:"enableTpu,omitempty" tf:"enable_tpu"`
	// The IP address of this cluster's Kubernetes master.
	// +optional
	Endpoint *string `json:"endpoint,omitempty" tf:"endpoint"`
	// The number of nodes to create in this cluster's default node pool. In regional or multi-zonal clusters, this is the number of nodes per zone. Must be set if node_pool is not set. If you're using google_container_node_pool objects with no default node pool, you'll need to set this to a value of at least 1, alongside setting remove_default_node_pool to true.
	// +optional
	InitialNodeCount *int64 `json:"initialNodeCount,omitempty" tf:"initial_node_count"`
	// List of instance group URLs which have been assigned to the cluster.
	// +optional
	InstanceGroupUrls []string `json:"instanceGroupUrls,omitempty" tf:"instance_group_urls"`
	// Configuration of cluster IP allocation for VPC-native clusters. Adding this block enables IP aliasing, making the cluster VPC-native instead of routes-based.
	// +optional
	IpAllocationPolicy *ClusterSpecIpAllocationPolicy `json:"ipAllocationPolicy,omitempty" tf:"ip_allocation_policy"`
	// The fingerprint of the set of labels for this cluster.
	// +optional
	LabelFingerprint *string `json:"labelFingerprint,omitempty" tf:"label_fingerprint"`
	// The location (region or zone) in which the cluster master will be created, as well as the default node location. If you specify a zone (such as us-central1-a), the cluster will be a zonal cluster with a single cluster master. If you specify a region (such as us-west1), the cluster will be a regional cluster with multiple masters spread across zones in the region, and with default node locations in those zones as well.
	// +optional
	Location *string `json:"location,omitempty" tf:"location"`
	// The logging service that the cluster should write logs to. Available options include logging.googleapis.com(Legacy Stackdriver), logging.googleapis.com/kubernetes(Stackdriver Kubernetes Engine Logging), and none. Defaults to logging.googleapis.com/kubernetes.
	// +optional
	LoggingService *string `json:"loggingService,omitempty" tf:"logging_service"`
	// The maintenance policy to use for the cluster.
	// +optional
	MaintenancePolicy *ClusterSpecMaintenancePolicy `json:"maintenancePolicy,omitempty" tf:"maintenance_policy"`
	// The authentication information for accessing the Kubernetes master. Some values in this block are only returned by the API if your service account has permission to get credentials for your GKE cluster. If you see an unexpected diff removing a username/password or unsetting your client cert, ensure you have the container.clusters.getCredentials permission.
	// +optional
	MasterAuth *ClusterSpecMasterAuth `json:"masterAuth,omitempty" tf:"master_auth"`
	// The desired configuration options for master authorized networks. Omit the nested cidr_blocks attribute to disallow external access (except the cluster node IPs, which GKE automatically whitelists).
	// +optional
	MasterAuthorizedNetworksConfig *ClusterSpecMasterAuthorizedNetworksConfig `json:"masterAuthorizedNetworksConfig,omitempty" tf:"master_authorized_networks_config"`
	// The current version of the master in the cluster. This may be different than the min_master_version set in the config if the master has been updated by GKE.
	// +optional
	MasterVersion *string `json:"masterVersion,omitempty" tf:"master_version"`
	// The minimum version of the master. GKE will auto-update the master to new versions, so this does not guarantee the current master version--use the read-only master_version field to obtain that. If unset, the cluster's version will be set by GKE to the version of the most recent official release (which is not necessarily the latest version).
	// +optional
	MinMasterVersion *string `json:"minMasterVersion,omitempty" tf:"min_master_version"`
	// The monitoring service that the cluster should write metrics to. Automatically send metrics from pods in the cluster to the Google Cloud Monitoring API. VM metrics will be collected by Google Compute Engine regardless of this setting Available options include monitoring.googleapis.com(Legacy Stackdriver), monitoring.googleapis.com/kubernetes(Stackdriver Kubernetes Engine Monitoring), and none. Defaults to monitoring.googleapis.com/kubernetes.
	// +optional
	MonitoringService *string `json:"monitoringService,omitempty" tf:"monitoring_service"`
	// The name of the cluster, unique within the project and location.
	Name *string `json:"name" tf:"name"`
	// The name or self_link of the Google Compute Engine network to which the cluster is connected. For Shared VPC, set this to the self link of the shared network.
	// +optional
	Network *string `json:"network,omitempty" tf:"network"`
	// Configuration options for the NetworkPolicy feature.
	// +optional
	NetworkPolicy *ClusterSpecNetworkPolicy `json:"networkPolicy,omitempty" tf:"network_policy"`
	// Determines whether alias IPs or routes will be used for pod IPs in the cluster.
	// +optional
	NetworkingMode *string `json:"networkingMode,omitempty" tf:"networking_mode"`
	// The configuration of the nodepool
	// +optional
	NodeConfig *ClusterSpecNodeConfig `json:"nodeConfig,omitempty" tf:"node_config"`
	// The list of zones in which the cluster's nodes are located. Nodes must be in the region of their regional cluster or in the same region as their cluster's zone for zonal clusters. If this is specified for a zonal cluster, omit the cluster's zone.
	// +optional
	NodeLocations []string `json:"nodeLocations,omitempty" tf:"node_locations"`
	// List of node pools associated with this cluster. See google_container_node_pool for schema. Warning: node pools defined inside a cluster can't be changed (or added/removed) after cluster creation without deleting and recreating the entire cluster. Unless you absolutely need the ability to say "these are the only node pools associated with this cluster", use the google_container_node_pool resource instead of this property.
	// +optional
	NodePool []ClusterSpecNodePool `json:"nodePool,omitempty" tf:"node_pool"`
	// The Kubernetes version on the nodes. Must either be unset or set to the same value as min_master_version on create. Defaults to the default version set by GKE which is not necessarily the latest version. This only affects nodes in the default node pool. While a fuzzy version can be specified, it's recommended that you specify explicit versions as Terraform will see spurious diffs when fuzzy versions are used. See the google_container_engine_versions data source's version_prefix field to approximate fuzzy versions in a Terraform-compatible way. To update nodes in other node pools, use the version attribute on the node pool.
	// +optional
	NodeVersion *string `json:"nodeVersion,omitempty" tf:"node_version"`
	// +optional
	Operation *string `json:"operation,omitempty" tf:"operation"`
	// Configuration for the PodSecurityPolicy feature.
	// +optional
	PodSecurityPolicyConfig *ClusterSpecPodSecurityPolicyConfig `json:"podSecurityPolicyConfig,omitempty" tf:"pod_security_policy_config"`
	// Configuration for private clusters, clusters with private nodes.
	// +optional
	PrivateClusterConfig *ClusterSpecPrivateClusterConfig `json:"privateClusterConfig,omitempty" tf:"private_cluster_config"`
	// The desired state of IPv6 connectivity to Google Services. By default, no private IPv6 access to or from Google Services (all access will be via IPv4).
	// +optional
	PrivateIpv6GoogleAccess *string `json:"privateIpv6GoogleAccess,omitempty" tf:"private_ipv6_google_access"`
	// The ID of the project in which the resource belongs. If it is not provided, the provider project is used.
	// +optional
	Project *string `json:"project,omitempty" tf:"project"`
	// Configuration options for the Release channel feature, which provide more control over automatic upgrades of your GKE clusters. Note that removing this field from your config will not unenroll it. Instead, use the "UNSPECIFIED" channel.
	// +optional
	ReleaseChannel *ClusterSpecReleaseChannel `json:"releaseChannel,omitempty" tf:"release_channel"`
	// If true, deletes the default node pool upon cluster creation. If you're using google_container_node_pool resources with no default node pool, this should be set to true, alongside setting initial_node_count to at least 1.
	// +optional
	RemoveDefaultNodePool *bool `json:"removeDefaultNodePool,omitempty" tf:"remove_default_node_pool"`
	// The GCE resource labels (a map of key/value pairs) to be applied to the cluster.
	// +optional
	ResourceLabels *map[string]string `json:"resourceLabels,omitempty" tf:"resource_labels"`
	// Configuration for the ResourceUsageExportConfig feature.
	// +optional
	ResourceUsageExportConfig *ClusterSpecResourceUsageExportConfig `json:"resourceUsageExportConfig,omitempty" tf:"resource_usage_export_config"`
	// Server-defined URL for the resource.
	// +optional
	SelfLink *string `json:"selfLink,omitempty" tf:"self_link"`
	// The IP address range of the Kubernetes services in this cluster, in CIDR notation (e.g. 1.2.3.4/29). Service addresses are typically put in the last /16 from the container CIDR.
	// +optional
	ServicesIpv4CIDR *string `json:"servicesIpv4CIDR,omitempty" tf:"services_ipv4_cidr"`
	// The name or self_link of the Google Compute Engine subnetwork in which the cluster's instances are launched.
	// +optional
	Subnetwork *string `json:"subnetwork,omitempty" tf:"subnetwork"`
	// The IP address range of the Cloud TPUs in this cluster, in CIDR notation (e.g. 1.2.3.4/29).
	// +optional
	TpuIpv4CIDRBlock *string `json:"tpuIpv4CIDRBlock,omitempty" tf:"tpu_ipv4_cidr_block"`
	// Vertical Pod Autoscaling automatically adjusts the resources of pods controlled by it.
	// +optional
	VerticalPodAutoscaling *ClusterSpecVerticalPodAutoscaling `json:"verticalPodAutoscaling,omitempty" tf:"vertical_pod_autoscaling"`
	// Configuration for the use of Kubernetes Service Accounts in GCP IAM policies.
	// +optional
	WorkloadIdentityConfig *ClusterSpecWorkloadIdentityConfig `json:"workloadIdentityConfig,omitempty" tf:"workload_identity_config"`
}

type ClusterStatus struct {
	// Resource generation, which is updated on mutation by the API Server.
	// +optional
	ObservedGeneration int64 `json:"observedGeneration,omitempty"`
	// +optional
	Phase base.Phase `json:"phase,omitempty"`
	// +optional
	Conditions []kmapi.Condition `json:"conditions,omitempty"`
}

// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object
// +kubebuilder:object:root=true

// ClusterList is a list of Clusters
type ClusterList struct {
	metav1.TypeMeta `json:",inline"`
	metav1.ListMeta `json:"metadata,omitempty"`
	// Items is a list of Cluster CRD objects
	Items []Cluster `json:"items,omitempty"`
}
