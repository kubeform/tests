/*
Copyright AppsCode Inc. and Contributors

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

// Code generated by Kubeform. DO NOT EDIT.

package v1alpha1

import (
	base "kubeform.dev/apimachinery/api/v1alpha1"

	core "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	kmapi "kmodules.xyz/client-go/api/v1"
)

// +genclient
// +k8s:openapi-gen=true
// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object
// +kubebuilder:object:root=true
// +kubebuilder:subresource:status
// +kubebuilder:printcolumn:name="Phase",type=string,JSONPath=`.status.phase`

type NodePool struct {
	metav1.TypeMeta   `json:",inline,omitempty"`
	metav1.ObjectMeta `json:"metadata,omitempty"`
	Spec              NodePoolSpec   `json:"spec,omitempty"`
	Status            NodePoolStatus `json:"status,omitempty"`
}

type NodePoolSpec struct {
	NodePoolSpec2 `json:",inline"`
	// +optional
	KubeformOutput NodePoolSpec2 `json:"kubeformOutput,omitempty" tf:"-"`
}

type NodePoolSpecAutoscaling struct {
	// Maximum number of nodes in the NodePool. Must be >= min_node_count.
	MaxNodeCount *int64 `json:"maxNodeCount" tf:"max_node_count"`
	// Minimum number of nodes in the NodePool. Must be >=0 and <= max_node_count.
	MinNodeCount *int64 `json:"minNodeCount" tf:"min_node_count"`
}

type NodePoolSpecManagement struct {
	// Whether the nodes will be automatically repaired.
	// +optional
	AutoRepair *bool `json:"autoRepair,omitempty" tf:"auto_repair"`
	// Whether the nodes will be automatically upgraded.
	// +optional
	AutoUpgrade *bool `json:"autoUpgrade,omitempty" tf:"auto_upgrade"`
}

type NodePoolSpecNodeConfigGuestAccelerator struct {
	// The number of the accelerator cards exposed to an instance.
	Count *int64 `json:"count" tf:"count"`
	// The accelerator type resource name.
	Type *string `json:"type" tf:"type"`
}

type NodePoolSpecNodeConfigShieldedInstanceConfig struct {
	// Defines whether the instance has integrity monitoring enabled.
	// +optional
	EnableIntegrityMonitoring *bool `json:"enableIntegrityMonitoring,omitempty" tf:"enable_integrity_monitoring"`
	// Defines whether the instance has Secure Boot enabled.
	// +optional
	EnableSecureBoot *bool `json:"enableSecureBoot,omitempty" tf:"enable_secure_boot"`
}

type NodePoolSpecNodeConfigTaint struct {
	// Effect for taint.
	Effect *string `json:"effect" tf:"effect"`
	// Key for taint.
	Key *string `json:"key" tf:"key"`
	// Value for taint.
	Value *string `json:"value" tf:"value"`
}

type NodePoolSpecNodeConfigWorkloadMetadataConfig struct {
	// NodeMetadata is the configuration for how to expose metadata to the workloads running on the node.
	NodeMetadata *string `json:"nodeMetadata" tf:"node_metadata"`
}

type NodePoolSpecNodeConfig struct {
	// Size of the disk attached to each node, specified in GB. The smallest allowed disk size is 10GB.
	// +optional
	DiskSizeGb *int64 `json:"diskSizeGb,omitempty" tf:"disk_size_gb"`
	// Type of the disk attached to each node.
	// +optional
	DiskType *string `json:"diskType,omitempty" tf:"disk_type"`
	// List of the type and count of accelerator cards attached to the instance.
	// +optional
	GuestAccelerator []NodePoolSpecNodeConfigGuestAccelerator `json:"guestAccelerator,omitempty" tf:"guest_accelerator"`
	// The image type to use for this node. Note that for a given image type, the latest version of it will be used.
	// +optional
	ImageType *string `json:"imageType,omitempty" tf:"image_type"`
	// The map of Kubernetes labels (key/value pairs) to be applied to each node. These will added in addition to any default label(s) that Kubernetes may apply to the node.
	// +optional
	Labels *map[string]string `json:"labels,omitempty" tf:"labels"`
	// The number of local SSD disks to be attached to the node.
	// +optional
	LocalSsdCount *int64 `json:"localSsdCount,omitempty" tf:"local_ssd_count"`
	// The name of a Google Compute Engine machine type.
	// +optional
	MachineType *string `json:"machineType,omitempty" tf:"machine_type"`
	// The metadata key/value pairs assigned to instances in the cluster.
	// +optional
	Metadata *map[string]string `json:"metadata,omitempty" tf:"metadata"`
	// Minimum CPU platform to be used by this instance. The instance may be scheduled on the specified or newer CPU platform.
	// +optional
	MinCPUPlatform *string `json:"minCPUPlatform,omitempty" tf:"min_cpu_platform"`
	// The set of Google API scopes to be made available on all of the node VMs.
	// +optional
	OauthScopes []string `json:"oauthScopes,omitempty" tf:"oauth_scopes"`
	// Whether the nodes are created as preemptible VM instances.
	// +optional
	Preemptible *bool `json:"preemptible,omitempty" tf:"preemptible"`
	// The Google Cloud Platform Service Account to be used by the node VMs.
	// +optional
	ServiceAccount *string `json:"serviceAccount,omitempty" tf:"service_account"`
	// Shielded Instance options.
	// +optional
	ShieldedInstanceConfig *NodePoolSpecNodeConfigShieldedInstanceConfig `json:"shieldedInstanceConfig,omitempty" tf:"shielded_instance_config"`
	// The list of instance tags applied to all nodes.
	// +optional
	Tags []string `json:"tags,omitempty" tf:"tags"`
	// List of Kubernetes taints to be applied to each node.
	// +optional
	Taint []NodePoolSpecNodeConfigTaint `json:"taint,omitempty" tf:"taint"`
	// The workload metadata configuration for this node.
	// +optional
	WorkloadMetadataConfig *NodePoolSpecNodeConfigWorkloadMetadataConfig `json:"workloadMetadataConfig,omitempty" tf:"workload_metadata_config"`
}

type NodePoolSpecUpgradeSettings struct {
	// The number of additional nodes that can be added to the node pool during an upgrade. Increasing max_surge raises the number of nodes that can be upgraded simultaneously. Can be set to 0 or greater.
	MaxSurge *int64 `json:"maxSurge" tf:"max_surge"`
	// The number of nodes that can be simultaneously unavailable during an upgrade. Increasing max_unavailable raises the number of nodes that can be upgraded in parallel. Can be set to 0 or greater.
	MaxUnavailable *int64 `json:"maxUnavailable" tf:"max_unavailable"`
}

type NodePoolSpec2 struct {
	TerminationPolicy base.TerminationPolicy `json:"terminationPolicy,omitempty" tf:"-"`

	Timeouts *base.ResourceTimeout `json:"timeouts,omitempty" tf:"timeouts"`

	ProviderRef core.LocalObjectReference `json:"providerRef" tf:"-"`

	ID string `json:"id,omitempty" tf:"id,omitempty"`

	// Configuration required by cluster autoscaler to adjust the size of the node pool to the current cluster usage.
	// +optional
	Autoscaling *NodePoolSpecAutoscaling `json:"autoscaling,omitempty" tf:"autoscaling"`
	// The cluster to create the node pool for. Cluster must be present in location provided for zonal clusters.
	Cluster *string `json:"cluster" tf:"cluster"`
	// The initial number of nodes for the pool. In regional or multi-zonal clusters, this is the number of nodes per zone. Changing this will force recreation of the resource.
	// +optional
	InitialNodeCount *int64 `json:"initialNodeCount,omitempty" tf:"initial_node_count"`
	// The resource URLs of the managed instance groups associated with this node pool.
	// +optional
	InstanceGroupUrls []string `json:"instanceGroupUrls,omitempty" tf:"instance_group_urls"`
	// The location (region or zone) of the cluster.
	// +optional
	Location *string `json:"location,omitempty" tf:"location"`
	// Node management configuration, wherein auto-repair and auto-upgrade is configured.
	// +optional
	Management *NodePoolSpecManagement `json:"management,omitempty" tf:"management"`
	// The maximum number of pods per node in this node pool. Note that this does not work on node pools which are "route-based" - that is, node pools belonging to clusters that do not have IP Aliasing enabled.
	// +optional
	MaxPodsPerNode *int64 `json:"maxPodsPerNode,omitempty" tf:"max_pods_per_node"`
	// The name of the node pool. If left blank, Terraform will auto-generate a unique name.
	// +optional
	Name *string `json:"name,omitempty" tf:"name"`
	// Creates a unique name for the node pool beginning with the specified prefix. Conflicts with name.
	// +optional
	NamePrefix *string `json:"namePrefix,omitempty" tf:"name_prefix"`
	// The configuration of the nodepool
	// +optional
	NodeConfig *NodePoolSpecNodeConfig `json:"nodeConfig,omitempty" tf:"node_config"`
	// The number of nodes per instance group. This field can be used to update the number of nodes per instance group but should not be used alongside autoscaling.
	// +optional
	NodeCount *int64 `json:"nodeCount,omitempty" tf:"node_count"`
	// The list of zones in which the node pool's nodes should be located. Nodes must be in the region of their regional cluster or in the same region as their cluster's zone for zonal clusters. If unspecified, the cluster-level node_locations will be used.
	// +optional
	NodeLocations []string `json:"nodeLocations,omitempty" tf:"node_locations"`
	// +optional
	Operation *string `json:"operation,omitempty" tf:"operation"`
	// The ID of the project in which to create the node pool. If blank, the provider-configured project will be used.
	// +optional
	Project *string `json:"project,omitempty" tf:"project"`
	// Specify node upgrade settings to change how many nodes GKE attempts to upgrade at once. The number of nodes upgraded simultaneously is the sum of max_surge and max_unavailable. The maximum number of nodes upgraded simultaneously is limited to 20.
	// +optional
	UpgradeSettings *NodePoolSpecUpgradeSettings `json:"upgradeSettings,omitempty" tf:"upgrade_settings"`
	// The Kubernetes version for the nodes in this pool. Note that if this field and auto_upgrade are both specified, they will fight each other for what the node version should be, so setting both is highly discouraged. While a fuzzy version can be specified, it's recommended that you specify explicit versions as Terraform will see spurious diffs when fuzzy versions are used. See the google_container_engine_versions data source's version_prefix field to approximate fuzzy versions in a Terraform-compatible way.
	// +optional
	Version *string `json:"version,omitempty" tf:"version"`
}

type NodePoolStatus struct {
	// Resource generation, which is updated on mutation by the API Server.
	// +optional
	ObservedGeneration int64 `json:"observedGeneration,omitempty"`
	// +optional
	Phase base.Phase `json:"phase,omitempty"`
	// +optional
	Conditions []kmapi.Condition `json:"conditions,omitempty"`
}

// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object
// +kubebuilder:object:root=true

// NodePoolList is a list of NodePools
type NodePoolList struct {
	metav1.TypeMeta `json:",inline"`
	metav1.ListMeta `json:"metadata,omitempty"`
	// Items is a list of NodePool CRD objects
	Items []NodePool `json:"items,omitempty"`
}
